# Benchmark Convs
Here is the reference sample training scripts for convolutions. 

### Notice
Since we don't implement the NHWC logic for the batchnormalization, if you direcly choose the channel_last format for training, it will reproduce wrong result.
Thus, here we recommend to run the trail in the following setup
- Set `export QSYNC_SIMU=1` when you want to get the accuracy information. QDQ will be applied to approximate the training accuracy result. 
- Set `export QSYNC_SIMU=0` when you want to get the benchmark infos.

Noticed that the performance of our training will be slower than the `NCHW` version (about 0.5x throughput), but comparable to the official speed of the `NHWC` provided by the PyTorch.

### Profiler
We provides four `profile_*.sh` scripts for profiling. The profiled result will help build the local and global DFG of the training.
- Specially, the `profile_only_*.sh` profile the statistical result used for indicator
- `porofile_*.sh` profile the latency for kernels used for local and global DFG construction

Alert! pls remove `nn.utils.clip_grad_norm_` when doing the profiling. 
We use gradient clipping to avoid the NAN in training of VGG16. As Predictor only consider a most basic training flow (fwd, bwd, optimizer step) for synchronization, pls don't specify it in the profiling. 


### Load Bitwidth Result
The bitwidth result is loaded through specify the `--mapper_path`. e.g. `--mapper_path a.npy`, where `a.npy` is generated by the syncer. 
- since user need to specified the last layer (classifier)'s name in `QSync.LpTorch.config`, we initially provides four module conversion rejection name(which means this module will not be converted to mixed-bitwidth version), while the `roberta` and `vgg16` will encountered for the `classifier`, thus you need to comments the `classifier` when load the bitwidth setup for the vgg16.


### Trails
We provides sample trails to use LpTorch for training. 
- e.g. specify `--int8` to convert all conv-layers to `int8` kernel and other linear-layer (ReLu, Maxpool2d) to fp16. 
- see more in test cases we provided in the `main.py`